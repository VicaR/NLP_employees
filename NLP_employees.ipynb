{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6, color=blue>NLP обработка данных о сотрудниках ВУЗа</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цель работы и исходные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из текста в CSV файле необходимо распарсить имя человека и его должность.\n",
    "\n",
    "В колонке json храниться 2 ключа “left” и “right” , при конкатенации этих  значений получается текст, в котором теоретически может храниться информация о сотруднике ВУЗ.\n",
    "\tНапример из вот такой строки:\n",
    "“\n",
    "Department Contact:\n",
    "Jodi Musser, Program Director\n",
    "509-963-2773\n",
    "“\n",
    "Необходимо вытащить информацию в таком виде:\n",
    "⦁\tfirst_name: Jodi\n",
    "⦁\tlast_name: Musser\n",
    "⦁\tjob_title: Program Director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исходные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⦁\tCSV файл с данными data.csv\n",
    "\n",
    "⦁\tсписок ключевиков с названиями потенциальных должностей https://drive.google.com/file/d/1vs3MQr-DvklQ9tmTQP9zYdDfV6fzPzBI/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Требования к выполнению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требования к выполнению задания:\n",
    "\n",
    "⦁\tПри разработке использовать библиотеку SpaCy\n",
    "\n",
    "⦁\tСкрипт должен работать максимально оптимизировано. Каждый новый запрос в модель должен выполняться < 1c.\n",
    "\n",
    "⦁\tПри поиске должностей следует использовать spacy.Matcher (https://spacy.io/usage/rule-based-matching) для выявления нечетких совпадений между заданными ключевиками и должностями в заданном тексте.\n",
    "\n",
    "⦁\tРезультатом работы должен быть тот же CSV файл, но с заполненными полями:\n",
    "⦁\tfirst_name\n",
    "⦁\tlast_name\n",
    "⦁\tjob_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследовательский анализ и чистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "#Import Matcher class\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "#Load model for english language\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка набора данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>email</th>\n",
       "      <th>json</th>\n",
       "      <th>title</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>academic_title</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>processed</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>vfenn@abac.edu</td>\n",
       "      <td>{\"left\": \" the winner. The number on each ball...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>bray@abac.edu</td>\n",
       "      <td>{\"left\": \"er person and can be purchased onlin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>admissions@abac.edu</td>\n",
       "      <td>{\"left\": \"ty, Prince Automotive Group, Rotary ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>webmaster@abac.edu</td>\n",
       "      <td>{\"left\": \"mics\\nRegistrar\\nTranscript Request\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.alu.edu/</td>\n",
       "      <td>admissions@alu.edu</td>\n",
       "      <td>{\"left\": \"Abraham Lincoln University &amp; Online ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    url                email  \\\n",
       "0   1  https://www.abac.edu/       vfenn@abac.edu   \n",
       "1   2  https://www.abac.edu/        bray@abac.edu   \n",
       "2   3  https://www.abac.edu/  admissions@abac.edu   \n",
       "3   4  https://www.abac.edu/   webmaster@abac.edu   \n",
       "4   5   https://www.alu.edu/   admissions@alu.edu   \n",
       "\n",
       "                                                json  title  first_name  \\\n",
       "0  {\"left\": \" the winner. The number on each ball...    NaN         NaN   \n",
       "1  {\"left\": \"er person and can be purchased onlin...    NaN         NaN   \n",
       "2  {\"left\": \"ty, Prince Automotive Group, Rotary ...    NaN         NaN   \n",
       "3  {\"left\": \"mics\\nRegistrar\\nTranscript Request\\...    NaN         NaN   \n",
       "4  {\"left\": \"Abraham Lincoln University & Online ...    NaN         NaN   \n",
       "\n",
       "   last_name  academic_title  department  school  processed  \\\n",
       "0        NaN             NaN         NaN     NaN        NaN   \n",
       "1        NaN             NaN         NaN     NaN        NaN   \n",
       "2        NaN             NaN         NaN     NaN        NaN   \n",
       "3        NaN             NaN         NaN     NaN        NaN   \n",
       "4        NaN             NaN         NaN     NaN        NaN   \n",
       "\n",
       "            created_at           updated_at  \n",
       "0  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "1  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "2  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "3  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "4  2019-09-16 11:37:24  2020-02-06 03:33:24  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"left\": \" the winner. The number on each ball will be associated with an\\\\nindividual that purchased chances to win. The grand prize is a check with a\\\\ndesignated value equal to 50 per cent (nearest $10) of the numbered golf balls\\\\nsold.\\\\nTo\\\\nparticipate in the tournament or the ball drop event, interested persons can\\\\ncontact Fenn at (229) 391-5067, email her at \", \"right\": \"\\\\n, or register online at https://www.abac.edu/academics/sanr-classic/ .\\\\n###\\\\nBaldwin Players Announce Cast for ABAC Fall Production\\\\nSeptember 3 2019\\\\nBaldwin Players Announce Cast for ABAC Fall Production\\\\nTIFTONâ€”Baldwin\\\\nPlayersâ€™ Director Brian Ray has announced the cast for the theatre troupeâ€™s\\\\nupcoming production of â€œBoeing, Boeingâ€\\x9d by Mark Camolett\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0,'json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99924, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет содержит 99924 записи, 13 столбцов. Нас интересует столбец json. \n",
    "\n",
    "Проверим наличие пустых значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.json.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столбец не содержит пустых значений. Проверим по нему дубликаты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42573 duplicates\n"
     ]
    }
   ],
   "source": [
    "def duplicates(df):\n",
    "    dupl=df.duplicated(subset=['json'])\n",
    "    s=0\n",
    "    for ind in dupl.index:\n",
    "        if dupl.loc[ind]==True:\n",
    "            s+=1\n",
    "    print('{} duplicates'.format(s))\n",
    "duplicates(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку задача состоит в извлечении информации только из столбца 'json' без связи с данными других столбцов, то дубликаты по нему можно удалить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates\n",
    "data=data.drop_duplicates(subset=['json'], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57351, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После удаления дубликатов датасет содержит 57351 строку.\n",
    "\n",
    "Переименуем столбец 'title' в соответствии с заданием в 'job_title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'title':'job_title'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним конкатенацию данных в столбце 'json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-991f7defa32e>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['json'][idx]=json_concat\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for idx in data.index:\n",
    "    data_json = json.loads(data['json'][idx])\n",
    "    json_concat=data_json['left']+data_json['right']\n",
    "    data['json'][idx]=json_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>email</th>\n",
       "      <th>json</th>\n",
       "      <th>job_title</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>academic_title</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>processed</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>vfenn@abac.edu</td>\n",
       "      <td>the winner. The number on each ball will be a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>bray@abac.edu</td>\n",
       "      <td>er person and can be purchased online at www.p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>admissions@abac.edu</td>\n",
       "      <td>ty, Prince Automotive Group, Rotary Club of\\nT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>webmaster@abac.edu</td>\n",
       "      <td>mics\\nRegistrar\\nTranscript Request\\nAcademic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.alu.edu/</td>\n",
       "      <td>admissions@alu.edu</td>\n",
       "      <td>Abraham Lincoln University &amp; Online Law School...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    url                email  \\\n",
       "0   1  https://www.abac.edu/       vfenn@abac.edu   \n",
       "1   2  https://www.abac.edu/        bray@abac.edu   \n",
       "2   3  https://www.abac.edu/  admissions@abac.edu   \n",
       "3   4  https://www.abac.edu/   webmaster@abac.edu   \n",
       "4   5   https://www.alu.edu/   admissions@alu.edu   \n",
       "\n",
       "                                                json  job_title  first_name  \\\n",
       "0   the winner. The number on each ball will be a...        NaN         NaN   \n",
       "1  er person and can be purchased online at www.p...        NaN         NaN   \n",
       "2  ty, Prince Automotive Group, Rotary Club of\\nT...        NaN         NaN   \n",
       "3  mics\\nRegistrar\\nTranscript Request\\nAcademic ...        NaN         NaN   \n",
       "4  Abraham Lincoln University & Online Law School...        NaN         NaN   \n",
       "\n",
       "   last_name  academic_title  department  school  processed  \\\n",
       "0        NaN             NaN         NaN     NaN        NaN   \n",
       "1        NaN             NaN         NaN     NaN        NaN   \n",
       "2        NaN             NaN         NaN     NaN        NaN   \n",
       "3        NaN             NaN         NaN     NaN        NaN   \n",
       "4        NaN             NaN         NaN     NaN        NaN   \n",
       "\n",
       "            created_at           updated_at  \n",
       "0  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "1  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "2  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "3  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "4  2019-09-16 11:37:24  2020-02-06 03:33:24  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение данных из текста с помощью SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм работы с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определить список должностей, которые потенциально могут содержаться в тексте (он один для всех записей, задан в исходных данных)\n",
    "2. Определить перечень имён, которые встречаются в данном тексте (он уникален для каждой записи, его мы будем формировать с помощью распознавания имён - Named Entity Recognition)\n",
    "3. Составить паттерны \"должность+имя\" для данного текста\n",
    "4. С помощью Matcher найти эти паттерны в тексте\n",
    "5. Распарсить результат совпадений и поместить в заданные столбцы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя предоставленную в задании функцию, получим список потенциальных должностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keywords import keywords\n",
    "tokens=keywords()\n",
    "tokens=tokens['academic_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Distinguished Professor',\n",
       " 'Professor',\n",
       " 'Associate Professor',\n",
       " 'Assistant Professor',\n",
       " 'Adjunct Professor',\n",
       " 'Senior Lecturer',\n",
       " 'Lecturer',\n",
       " 'Associate Lecturer',\n",
       " 'Assistant Lecturer',\n",
       " 'Professor Emeritus',\n",
       " 'Emeritus Professor',\n",
       " 'Dean',\n",
       " 'Head',\n",
       " 'Chair',\n",
       " 'Director',\n",
       " 'Provost',\n",
       " 'Instructor',\n",
       " 'Coach',\n",
       " 'Coordinator',\n",
       " 'Manager',\n",
       " 'Counselor',\n",
       " 'Trainer',\n",
       " 'Cashier',\n",
       " 'President',\n",
       " 'Researcher',\n",
       " 'Assistant',\n",
       " 'Research Associate',\n",
       " 'Counselor',\n",
       " 'Adviser',\n",
       " 'VP',\n",
       " 'Analyst',\n",
       " 'Officer',\n",
       " 'Chief']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке есть должности как состоящие из одного слова, так и из двух. Поэтому при составлении паттернов нужно будет учесть оба варианта. Для этого из списка tokens сделаем 3 разных списка: , первый с первым словом из двухсложных названий, второй - со вторым словом из них, третий с однословными должностями. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_titles: ['Distinguished', 'Associate', 'Assistant', 'Adjunct', 'Senior', 'Associate', 'Assistant', 'Professor', 'Emeritus', 'Research']\n",
      "last_titles:  ['Professor', 'Professor', 'Professor', 'Professor', 'Lecturer', 'Lecturer', 'Lecturer', 'Emeritus', 'Professor', 'Associate']\n",
      "only_first_titles: ['Professor', 'Lecturer', 'Dean', 'Head', 'Chair', 'Director', 'Provost', 'Instructor', 'Coach', 'Coordinator', 'Manager', 'Counselor', 'Trainer', 'Cashier', 'President', 'Researcher', 'Assistant', 'Counselor', 'Adviser', 'VP', 'Analyst', 'Officer', 'Chief']\n"
     ]
    }
   ],
   "source": [
    "first_titles=[title.split(' ')[0] for title in tokens if ' ' in title]\n",
    "last_titles=[title.split(' ')[1] for title in tokens if ' ' in title]\n",
    "only_first_titles=[title for title in tokens if ' ' not in title]\n",
    "\n",
    "print('first_titles:', first_titles) \n",
    "print('last_titles: ',last_titles)\n",
    "print('only_first_titles:',only_first_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение паттернов из одной записи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При написании скрипта следует учесть возможные варианты:\n",
    "1. Запись не содержит ни должностей, ни имён.\n",
    "2. Запись не содержит должностей, но содержит имена.\n",
    "3. Запись содержит должность(и), но без имени\n",
    "4. Запись содержит должность+имя\n",
    "5. Запись содержит несколько паттернов \"должность+имя\"\n",
    "\n",
    "По смыслу задания будем брать в рассмотрение только варианты 4 и 5. Для случаев 1-3 мы будем заполнять поля значениями 'Not found'. \n",
    "\n",
    "Кроме того, следует учесть, что имена также могут состоять только из одного слова или двух (трёхсложный вариант пока не рассматриваем).\n",
    "\n",
    "В случае варианта 5 примем решение заносить последовательность извлечённых должностей (список) в ячейку соотвествующей строки столбца 'job_title'. Аналогично поступим с именами, отвечающими этим должностям. Таким образом в результате выполнения скрипта мы заполняем 3 листа (job_title, first_name, last_name), элементами которых тоже будут листы. \n",
    "\n",
    "Также для начала согласимся, что если найденная моделью сущность \"PERSON\" состоит из одного слова, то это будет  first_name.  В данном случае в last_name мы заносим 'Not found'. После обработки датасета можно проанализировать полученные данные и поменять этот момент.\n",
    "\n",
    "В первом приближении будем искать только паттерны вида \"должность имя\" (как в задании). При необходимости можно будет создать более сложные, например: \"{токен из должностей} of {токен из областей науки} имя\". \n",
    "В скрипт также добавлены команды для замера времени выполнения запроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brian', 'Mark']\n",
      "['Ray', 'Camolett']\n",
      "['Fenn']\n",
      "120 123 Director Brian Ray\n",
      "--- 0.03298139572143555 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Lists to store the extracted data\n",
    "l=len(data)\n",
    "job_title=[[] for _ in range(l)]\n",
    "data_first_names=[[] for _ in range(l)]\n",
    "data_last_names=[[] for _ in range(l)]\n",
    "spans=[]\n",
    "\n",
    "#Script timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#Test the script on the first row of dataset\n",
    "index_of_text_to_test_on = 0\n",
    "text_to_test_on = data.json.iloc[index_of_text_to_test_on]\n",
    "idx=index_of_text_to_test_on \n",
    "\n",
    "#Process text with nlp-model\n",
    "doc = nlp(text_to_test_on)\n",
    "\n",
    "#Recognize named entities in doc and create lists of names\n",
    "persons=[entity.text for entity in doc.ents if entity.label_==\"PERSON\"]\n",
    "            \n",
    "first_names=[person.split(' ')[0] for person in persons if ' ' in person]\n",
    "last_names=[person.split(' ')[1] for person in persons if ' ' in person]\n",
    "only_first_names=[person for person in persons if ' ' not in person]\n",
    "\n",
    "print(first_names) \n",
    "print(last_names)\n",
    "print(only_first_names)  \n",
    "\n",
    "#Create Matcher object        \n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#Create the patterns:\n",
    "#1-word title+first_name+last_name\n",
    "pattern1 = [{\"TEXT\": {\"IN\": only_first_titles}},{\"TEXT\":{\"IN\": first_names}},{\"TEXT\":{\"IN\":last_names}}]\n",
    "#1-word title+1-word name (first_name)\n",
    "pattern2=[{\"TEXT\": {\"IN\": only_first_titles}},{\"TEXT\":{\"IN\": only_first_names}}]\n",
    "#2-words title+first_name+last_name\n",
    "pattern3 = [{\"TEXT\": {\"IN\": first_titles}},{\"TEXT\": {\"IN\": last_titles}},{\"TEXT\":{\"IN\": first_names}},{\"TEXT\":{\"IN\":last_names}}]\n",
    "#2-words title+1-word name (first_name)\n",
    "pattern4=[{\"TEXT\": {\"IN\": first_titles}},{\"TEXT\": {\"IN\": last_titles}},{\"TEXT\":{\"IN\": only_first_names}}]\n",
    "\n",
    "#Add pattern rules to the matcher\n",
    "matcher.add(\"JSON\", None, pattern1,pattern2,pattern3,pattern4)\n",
    "\n",
    "#Find matches\n",
    "matches = matcher(doc)\n",
    "\n",
    "#Parse results to corrresponding lists\n",
    "if not matches:\n",
    "                job_title[idx]='Not found'\n",
    "                data_first_names[idx]='Not found'\n",
    "                data_last_names[idx]='Not found'\n",
    "else:\n",
    "                spans = [doc[start:end] for _, start, end in matches]\n",
    "                for span in spacy.util.filter_spans(spans): \n",
    "                    print(span.start, span.end, span.text)\n",
    "\n",
    "                    if span.end-span.start==2:  #1-word job-title, 1-word name (only first name)\n",
    "                        job_title[idx].append(doc[span.start])\n",
    "                        data_first_names[idx].append(doc[span.start+1]) \n",
    "                        data_last_names[idx].append('Not found')  \n",
    "                    elif span.end-span.start==4:  #2-words job-title, 2-words name (first name, last name)\n",
    "                        job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                        data_first_names[idx].append(doc[span.start+2]) \n",
    "                        data_last_names[idx].append(doc[span.start+3])\n",
    "                    else:  #Check if 2 first words are name of job_title             \n",
    "                        if str(doc[span.start:span.start+2]) in tokens:\n",
    "                            #2-word job-title, 1-words name (first name) \n",
    "                            job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                            data_first_names[idx].append(doc[span.start+2])  \n",
    "                            data_last_names[idx].append('Not found')  \n",
    "                        else:#1-word job-title, 2-words name (first name, last name)       \n",
    "                            job_title[idx].append(doc[span.start]) \n",
    "                            data_first_names[idx].append(doc[span.start+1]) \n",
    "                            data_last_names[idx].append(doc[span.start+2])    \n",
    "                                                  \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время выполнения 1 запроса (0.034 с) не превышает требуемого. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение данных из всего набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(data)\n",
    "job_title=[[] for _ in range(l)]\n",
    "data_first_names=[[] for _ in range(l)]\n",
    "data_last_names=[[] for _ in range(l)]\n",
    "spans=[]\n",
    "\n",
    "\n",
    "for idx,string in data.iterrows():\n",
    "        doc = nlp(string.json)\n",
    "\n",
    "        persons=[entity.text for entity in doc.ents if entity.label_==\"PERSON\"]   \n",
    "        first_names=[person.split(' ')[0] for person in persons if ' ' in person]\n",
    "        last_names=[person.split(' ')[1] for person in persons if ' ' in person]\n",
    "        only_first_names=[person for person in persons if ' ' not in person]\n",
    "\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        pattern1 = [{\"TEXT\": {\"IN\": only_first_titles}},{\"TEXT\":{\"IN\": first_names}},{\"TEXT\":{\"IN\":last_names}}]\n",
    "        pattern2=[{\"TEXT\": {\"IN\": only_first_titles}},{\"TEXT\":{\"IN\": only_first_names}}]\n",
    "        pattern3 = [{\"TEXT\": {\"IN\": first_titles}},{\"TEXT\": {\"IN\": last_titles}},{\"TEXT\":{\"IN\": first_names}},{\"TEXT\":{\"IN\":last_names}}]\n",
    "        pattern4=[{\"TEXT\": {\"IN\": first_titles}},{\"TEXT\": {\"IN\": last_titles}},{\"TEXT\":{\"IN\": only_first_names}}]\n",
    "\n",
    "        matcher.add(\"JSON\", None, pattern1,pattern2,pattern3,pattern4)\n",
    "\n",
    "        matches = matcher(doc)\n",
    "\n",
    "        if not matches:\n",
    "                    job_title[idx]='Not found'\n",
    "                    data_first_names[idx]='Not found'\n",
    "                    data_last_names[idx]='Not found'\n",
    "        else:\n",
    "                    spans = [doc[start:end] for _, start, end in matches]\n",
    "                    for span in spacy.util.filter_spans(spans): \n",
    "\n",
    "                        if span.end-span.start==2:  #1-word job-title, 1-word name (only first name)\n",
    "                            job_title[idx].append(doc[span.start])\n",
    "                            data_first_names[idx].append(doc[span.start+1]) \n",
    "                            data_last_names[idx].append('Not found')  \n",
    "                        elif span.end-span.start==4:  #2-words job-title, 2-words name (first name, last name)\n",
    "                            job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                            data_first_names[idx].append(doc[span.start+2]) \n",
    "                            data_last_names[idx].append(doc[span.start+3])\n",
    "                        else:  #Check if 2 first words are name of job_title             \n",
    "                            if str(doc[span.start:span.start+2]) in tokens:  #2-word job-title, 1-words name (first name) \n",
    "                                job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                                data_first_names[idx].append(doc[span.start+2])  \n",
    "                                data_last_names[idx].append('Not found')  \n",
    "                            else:#1-word job-title, 2-words name (first name, last name)       \n",
    "                                job_title[idx].append(doc[span.start]) \n",
    "                                data_first_names[idx].append(doc[span.start+1]) \n",
    "                                data_last_names[idx].append(doc[span.start+2])     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_title: 57351 data_first_names: 57351 data_last_names: 57351\n"
     ]
    }
   ],
   "source": [
    "#Check that lists have correct length                                \n",
    "print('job_title:',len(job_title),'data_first_names:',len(data_first_names),'data_last_names:',len(data_last_names))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распарсим полученные результаты в датасет. Для удобства скопируем исходный в новый data_filled, в который и поместим результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filled=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-21885e009796>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  else: data_filled['job_title'][idx]=', '.join([str(elem) for elem in job_title[idx]])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "<ipython-input-29-21885e009796>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  else: data_filled['first_name'][idx]=', '.join([str(elem) for elem in data_first_names[idx]])\n",
      "<ipython-input-29-21885e009796>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  else: data_filled['last_name'][idx]=', '.join([str(elem) for elem in data_last_names[idx]])\n",
      "<ipython-input-29-21885e009796>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filled['job_title'][idx]='Not found'\n",
      "<ipython-input-29-21885e009796>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filled['first_name'][idx]='Not found'\n",
      "<ipython-input-29-21885e009796>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filled['last_name'][idx]='Not found'\n"
     ]
    }
   ],
   "source": [
    "#Transform lists it into strings\n",
    "for idx in data_filled.index:\n",
    "    if job_title[idx]=='Not found':\n",
    "        data_filled['job_title'][idx]='Not found'\n",
    "    else: data_filled['job_title'][idx]=', '.join([str(elem) for elem in job_title[idx]]) \n",
    "    if data_first_names[idx]=='Not found':\n",
    "        data_filled['first_name'][idx]='Not found'\n",
    "    else: data_filled['first_name'][idx]=', '.join([str(elem) for elem in data_first_names[idx]]) \n",
    "    if data_last_names[idx]=='Not found':\n",
    "        data_filled['last_name'][idx]='Not found'\n",
    "    else: data_filled['last_name'][idx]=', '.join([str(elem) for elem in data_last_names[idx]])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>email</th>\n",
       "      <th>json</th>\n",
       "      <th>job_title</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>academic_title</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>processed</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>vfenn@abac.edu</td>\n",
       "      <td>the winner. The number on each ball will be a...</td>\n",
       "      <td>Director</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Ray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>bray@abac.edu</td>\n",
       "      <td>er person and can be purchased online at www.p...</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>admissions@abac.edu</td>\n",
       "      <td>ty, Prince Automotive Group, Rotary Club of\\nT...</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.abac.edu/</td>\n",
       "      <td>webmaster@abac.edu</td>\n",
       "      <td>mics\\nRegistrar\\nTranscript Request\\nAcademic ...</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.alu.edu/</td>\n",
       "      <td>admissions@alu.edu</td>\n",
       "      <td>Abraham Lincoln University &amp; Online Law School...</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>Not found</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 11:37:24</td>\n",
       "      <td>2020-02-06 03:33:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    url                email  \\\n",
       "0   1  https://www.abac.edu/       vfenn@abac.edu   \n",
       "1   2  https://www.abac.edu/        bray@abac.edu   \n",
       "2   3  https://www.abac.edu/  admissions@abac.edu   \n",
       "3   4  https://www.abac.edu/   webmaster@abac.edu   \n",
       "4   5   https://www.alu.edu/   admissions@alu.edu   \n",
       "\n",
       "                                                json  job_title first_name  \\\n",
       "0   the winner. The number on each ball will be a...   Director      Brian   \n",
       "1  er person and can be purchased online at www.p...  Not found  Not found   \n",
       "2  ty, Prince Automotive Group, Rotary Club of\\nT...  Not found  Not found   \n",
       "3  mics\\nRegistrar\\nTranscript Request\\nAcademic ...  Not found  Not found   \n",
       "4  Abraham Lincoln University & Online Law School...  Not found  Not found   \n",
       "\n",
       "   last_name  academic_title  department  school  processed  \\\n",
       "0        Ray             NaN         NaN     NaN        NaN   \n",
       "1  Not found             NaN         NaN     NaN        NaN   \n",
       "2  Not found             NaN         NaN     NaN        NaN   \n",
       "3  Not found             NaN         NaN     NaN        NaN   \n",
       "4  Not found             NaN         NaN     NaN        NaN   \n",
       "\n",
       "            created_at           updated_at  \n",
       "0  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "1  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "2  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "3  2019-09-16 11:37:24  2020-02-06 03:33:24  \n",
       "4  2019-09-16 11:37:24  2020-02-06 03:33:24  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем таблицу с результатами в csv файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filled.to_csv('data_filled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим количество записей, где не найдены должности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Not found' in job_title 54656 - 95.3%\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "for idx in data_filled.index:\n",
    "    if data_filled['job_title'][idx]=='Not found':\n",
    "        t=t+1\n",
    "        t_perc=100*t/l\n",
    "print ('\\'Not found\\' in job_title {0} - {1:2.1f}%'.format(t,t_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         57351\n",
       "unique          104\n",
       "top       Not found\n",
       "freq          54656\n",
       "Name: job_title, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filled['job_title'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдено 104 разных должности. Однако большой процент пропусков наводит на мысль о существовании в тексте паттернов, которые мы не учли. \n",
    "Например, можно проверить, есть ли в тексте сочетания \"Professor of\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5889\n"
     ]
    }
   ],
   "source": [
    "s=len([data['json'][idx] for idx in data.index if 'Professor of' in data['json'][idx]])\n",
    "print(s)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдено 5889 записей с заданной комбинацией слов, которые могут быть полезной информацией. Таким образом, необходимо дополнять модель более широкими вариациями паттернов. \n",
    "\n",
    "Оценим качество распознавания имён:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brian', 'Emeritus', 'James', 'James', 'Jalen', 'Todd, Manfred', 'Marion', 'Timothy', 'David', 'Karen', 'Upward', 'Upward', 'Upward', 'Upward', 'Ross', 'Dean', 'Dean, Registrar', 'Dean, Registrar', 'Dean, Registrar', 'Quinn, Quinn', 'Direc', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Randy', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Lat', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'CIS', 'CIS', 'CIS', 'CIS', 'CIS', 'CIS', 'Basebal', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Athletic', 'Athletic', 'Athletic, Men', 'Athletic, Men', 'Athletic, Men', 'Men', 'Men', 'Men', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Emeritus', 'Emeritus, Dean', 'Emeritus', 'Emeritus', 'Emeritus, Dean, Dean', 'Emeritus, Dean, Dean', 'Dean, Dean', 'Dean', 'Dean', 'Dean', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Rebecca', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link, Navig', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Link', 'Hilary', 'Hilary', 'Link', 'Lacrosse', 'Lacrosse', 'Lacrosse', 'Lacrosse', 'Lacrosse', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Emerita', 'Emerita, Office', 'Emerita, Office', 'Emerita, Office', 'Emerita, Office, Office', 'Office, Office', 'Office', 'Office', 'Office', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Emeritus, Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Office', 'Office', 'Office', 'Office', 'Direc', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Profes', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Hilary', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar, Basebal', 'Stephens', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'John', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Forrest', 'Registrar, Registrar, Registrar', 'Liz', 'Liz', 'Human', '101', 'Gretchen, Laura', 'Gretchen, Laura', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Professo', 'Vejea, Tamira', 'Vejea, Tamira', 'Human', 'Human', 'Human', 'Human', 'Human', 'Human', 'Emerita', 'Alumni', 'Judy, Langstaff, Judy', 'Helene', 'Helene', 'Registrar, Registrar', 'Registrar, Registrar', 'Registrar, Registrar', 'Registrar, Registrar', 'Registrar, Registrar', 'Registrar, Registrar', 'Registrar', 'Registrar', 'Dean', 'Dean', 'Dean', 'Dean', 'Nair', 'Dean', 'Dean', 'Dean', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'forÂ', 'forÂ', 'forÂ', 'Vol', 'Volleyball', 'Volleyball', 'Volleyball', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Dean', 'Dean', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'forÂ', 'forÂ', 'forÂ', 'forÂ', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Registrar', 'Barry, Marcus', 'Barry, Marcus', 'Barry, Marcus', 'Barry, Marcus', 'Clayton', 'Emeritus', 'Emerita', 'Emerita', 'Human', 'Human, Human', 'Human, Human', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Emerita', 'Emeritus', 'Athletic', 'Emeri', 'Emeritus', 'Profes', 'Emeritus', 'Emerita', 'Athletic', 'Emeritus', 'Emerita', 'Emeritus', 'Professo', 'Professo', 'Professo', 'Associate', '&, &, &, &', 'Albert', \"'s, 's, Wood\", \"'s, Wood\", 'Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'John', 'Organization', 'Organization', 'Emeritus', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'ofÂ', 'ofÂ', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Associate', 'Associate', 'Associate', 'Associate', 'Associate', 'Associate', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Lyle', 'Lyle', 'Lyle', 'Barreiro', 'Morris', 'Hensel', 'Rogers', 'Laura', 'Ceramics', 'Ford', 'Ford', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Ceramics', 'Tyler', 'Barreiro', 'Barreiro', 'Tyler', '-', 'Registrar', '|', '|', '|', 'Martin', 'Martin, Bryan, James', 'Martin, Bryan, James', 'Martin, Bryan, James', 'James', 'Ted', 'Jodi, Jo', 'Jodi, Joseph, Joseph', 'Jodi, Joseph, Joseph', 'Michael, Jacqueline', 'Michael, Jacqueline', 'Jodi, Joseph, Joseph', 'Julian', 'Raphael, Lawrence', 'Dan', 'Ronald', 'Michael', 'Rance', 'Talks', 'Vickie', 'Vickie', 'Robert', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Registrar', 'Profes', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Controller', 'Dean', 'Controller', 'Dean', 'Dean', 'Dean', 'Jim', 'Jim', 'Dean', 'Dean', 'Bursar, Bursar', 'Bursar, Bursar', 'Bursar, Bursar', 'Bursar, Bursar', 'Adult', 'Adult', 'Adult', 'Adult', 'Adult', 'Adult', 'Coach, Coach, Coach, Coach, Coach, Coach, Coach', 'Coach, Coach, Coach, Coach, Coach, Coach, Coach', 'Coach, Coach, Coach, Coach, Coach, Coach', 'Coach, Coach, Coach, Coach, Coach, Coach', 'Coach, Coach, Coach, Coach, Coach', 'Coach, Coach, Coach, Coach, Coach, Coach', 'Controller', 'Controller', 'Controller', 'Controller', 'Controller', 'Controller', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Apps', 'Apps', 'Apps', 'Paul', 'Eddie', 'Molen', 'Molen', 'Nurse', 'Registrar', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'Costin', 'Costin, Costin', 'Costin, Costin', 'Costin', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'J.', 'Barack', 'Amy', 'Joe', 'Joe', 'Jeffrey', 'Jeffrey', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'John', 'John', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita, Emerita', 'Emerita, Emerita', 'Emerita', 'Emerita, Emeritus', 'Emerita, Emeritus', 'Emeritus, Emerita', 'Emeritus, Emerita', 'Emerita, Emeritus, Emerita', 'Emeritus, Emerita, Emeritus', 'Emeritus, Emerita, Emeritus, Emeritus', 'Emerita, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus, Dean', 'Dean', 'Dean', 'Dean', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Dean', 'Dean', 'Tools', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Siraj', 'Koschmann', 'Ronald', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Ed', 'Dean', 'Dean', 'Dean', 'Jean', 'Center', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Canisius', 'Canisius', 'Canisius', 'Canisius', 'Canisius', 'Canisius', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'WNY', 'Controller', 'Controller', 'Controller, Emerita', 'Controller, Emerita', 'Controller, Emerita', 'Controller, Emerita', 'Emerita', 'Emerita', 'Emeritus', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic, Mens', 'Mens, Emerita', 'Mens, Emerita', 'Mens, Emerita', 'Mens, Emerita', 'Mens, Emerita', 'Mens, Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Womens', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Elizabeth', 'Elizabeth', 'Debra', 'Debra', 'Debra', 'Dean', 'Volleyball', 'Volleyball', 'Volleyball, Athletic', 'Volleyball, Athletic', 'Volleyball, Athletic', 'Volleyball, Athletic', 'Volleyball, Athletic', 'Volleyball, Athletic', 'Volleyball, Athletic', 'Athletic', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Melissa', 'Assi', 'Matthew', 'Matthew', 'Emeritus', 'Emeritus', 'Emeritus', 'Cathy', 'Cathy', 'Cathy', 'Cathy', 'Cathy', 'Cathy', 'Cathy', 'Cathy', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita', 'Emerita', 'Emerita', 'Blaire', 'Jenny', 'Controller', 'Controller', 'Controller', 'Ballard', 'Athletic', 'Registrar', 'Registrar', 'Registrar', 'Controller', 'Controller', 'Controller', 'Controller', 'Controller', 'John', 'Dean', 'Dean', 'Dean', 'Dean, Dean', 'Laura', 'Laura', 'Ballard', 'Steve, Steve', 'Dean', 'Dean', 'Dean', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'John, Kristen, Jianying', 'John, Kristen, Jianying, Jianying', 'Kristen, Jianying, Jianying', 'Jianying, Jianying, Chandan', 'Jianying, Chandan', 'Chandan, Frances', 'Chandan, Frances, Xiaowen', 'Frances, Xiaowen', 'Frances, Xiaowen', 'Vinay', 'Vinay', 'Vinay, Katharine', 'Vinay, Katharine, Alfred', 'Vinay, Katharine, Alfred, Chandan', 'Katharine, Alfred, Chandan', 'Alfred, Chandan', 'Chandan', 'Don, Greg', 'Don, Greg, Catherine', 'Don, Greg, Catherine, Catherine', 'Greg, Catherine, Catherine', 'Catherine, Catherine, Catherine, Michael', 'Catherine, Catherine, Michael', 'Catherine, Michael, Barbra', 'Michael, Barbra', 'Barbra', 'Bev', 'Spencer', 'Stephanie', 'Mark', 'Polina', 'Teac', 'Financia', 'Upward', 'Upward', 'Upward', 'Upward', 'Upward', 'Upward', 'Wallerstein, Wallerstein', 'Angela', 'Angela', 'Angela', 'Angela', 'Angela', 'Angela', 'Gary', 'Samantha', 'Upward', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Carrie', 'Carrie', 'Nietupski', 'Nietupski, Nietupski', 'Skip', 'Skip', 'Skip', 'Skip', 'Skip', 'Michelle', 'Michelle', 'Michelle', 'Vitali', 'Cashier, Ellen', 'Cashier, Ellen', 'ofÂ', 'Paul', 'Paul', 'Vamvakas', 'Vamvakas', 'McCormick', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Associate', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'John', 'John', 'John', 'John', 'John', 'John', 'John', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Flight, Flight', 'Flight, Flight', 'Flight, Flight', 'Flight, Flight', 'Flight, Flight', 'Flight, Flight', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Registrar', 'Registrar', 'Dean', 'Dean', 'Athletic', 'Dean', 'Dean', 'Justine', 'Justine', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'Jazzy', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'AAUP, AAUP', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'John, John', 'John, John', 'Carla', 'Sydney', 'Colby', 'Profes', 'Carol', 'Robert', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'ofÂ', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'RN', 'RN', 'RN', 'RN', 'RN', 'Football', 'Football', 'Football', 'Football', 'Football', 'Rogers, Diversity, Diversity', 'RN', 'RN', 'RN', 'RN', 'RN', 'Joseph', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Profes', 'Profes', 'Dean', 'Dean', 'Dean', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus', '-, -, -', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Carolyn', 'Football', 'Football', 'Football', 'Football', 'Football', 'Football', 'Accountant', 'Accountant', 'Accountant', 'Accountant', 'Accountant', 'Online', 'Online', 'Online', 'Online', 'Online', 'Online', 'Hialeah, Online', 'John', 'John', 'John', 'John', 'Emeritus', 'William', 'Bursar', 'Registrar', 'Registrar', 'Registrar', 'Registrar, Registrar', 'Registrar, Registrar', 'Registrar', 'Carlo', 'Carlo', 'Carlo', 'Dean', 'Dean', 'Dean', 'Betsy', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Qubein', 'Daniel', 'Qubein', 'Shauna', 'Emerit', 'Aaron', '-', '-', '-', '-', '-', '-, -, -', '-, -, -', '-, -, -', '-, -, -, -', '-, -, -, -', '-, -, -, -', 'Athletic', 'Athletic, Athletics', 'Athletic, Athletics', 'Athletic, Athletics', 'Athletic, Athletics', 'Athletic, Athletics', 'Athletics', '-', 'Alumni', 'Alumni', 'Alumni', 'Alumni', 'Alumni', 'Career', 'Career', 'Career', 'Career', '-', '-', '-', '-', '-', 'Dean', 'Dean', 'Dean', 'Dean', 'Volle', 'Athletics', 'Athletics', 'Athletics', 'Profes', '-', '-, -', '-, -', '-, -', '-, -', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Admini', 'Ft', 'Ft', 'Ft', 'Ft, Athletic', '-, Athletic', 'Athletic', 'Athletic', 'Athletic', '-', '-', '-', '-', '-', '-', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Professo', 'Administrative', 'Administrative', 'Administrative', 'Administrative', 'Administrative', 'Administrative', '-', '-', '-', '-', '-, -', '-, -', '-, -, -', '-, -, -', '-, -', '-, -', '-, -', '-', 'F.', 'F.', 'Stephen', 'Emeritus', 'Emeritus', 'Emeritus', 'Volleyball', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean, Dean', 'Dean, Dean, Dean', 'Dean, Dean, Dean', 'Dean', 'Dean', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Controller', 'Controller', 'Controller', 'Controller', 'Controller', 'Controller', 'Fire', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Linda', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Student', 'Student', 'Student', 'Student', 'Student', 'Login', 'Login', 'Kurt', 'Registrar', 'Kemper', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Volleyball', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic, Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Registrar', 'Anderson', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Men', 'Men', 'Men', 'Men', 'Men', 'Behrens', 'Behrens', 'Behrens', 'Behrens', 'Behrens', 'Behrens', 'Athletic', 'Athletic', 'Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Ferguson', 'Ferguson', 'Ferguson, Athletic', 'Ferguson, Athletic', 'Ferguson, Athletic', 'Ferguson, Athletic', 'Athletic', 'Athletic', 'Advancement', 'Advancement', 'Advancement', 'Advancement', 'Advancement', 'Advancement', 'Soccer', 'Soccer', 'Soccer', 'Soccer', 'Soccer', 'Soccer', 'Vic', 'Leadership', 'Leadership', 'Leadership', 'Leadership', 'Leadership', 'Dean', 'John', 'Registrar', 'Registrar', 'Registrar, Dean', 'Registrar, Dean', 'Registrar, Dean', 'Registrar, Dean', 'Registrar, Dean', 'Dean', 'Dean', 'Dean', 'Registrar', 'Registrar', 'Nathaniel', 'Thomas', 'Heidi', 'Drinko', 'Jo', 'John', 'John, Sammy', 'Scott', 'Associate, Dean', 'Glover', 'Dennis', 'Bass', 'Bass', 'Emeritus', 'Carlos', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Dean', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita', 'Duncan', 'Dean', 'Dean', 'John, John, John', 'Volleyball', 'Carol', 'Chelsey', 'Chelsey', 'Diversity', 'Offi', 'Rundell, Rundell', 'Lawrence', 'Departmental', 'Carrie', 'Carrie', 'Carrie', 'Carrie', 'Carrie', 'Carrie', 'Amanda', 'Laura', 'Dean', 'Greg', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Registrar', 'Neal', 'Josephine', 'ofÂ, ofÂ', 'Ed', 'Dean', 'Emerita', 'ITC', 'Katherine', 'Dudley', 'Louis', 'Emeritus', 'Stacy', 'Larry', 'Kristin', 'Dean, Dean', 'Nicholson', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Stephen', 'W., Robin', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott, Carr', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Scott', 'Gornick', 'Eric', 'Emerita', 'Emerita', 'Athletic', 'Dean', 'Webmaster', 'Leo', 'Preet', 'Emeritus, Emeritus, Emeritus', 'Emeritus', 'Bishop', 'Benjamin', 'Hahn, Hahn', 'Elect', 'Beverly', 'Michael', 'Profes', 'Athletic, ofÂ', 'Athletic, ofÂ', 'Athletic, ofÂ', 'Lesley', 'Paul', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'NU201EW', 'Dean', 'Public', 'Hammond', 'Hammond', 'III, Dean', 'Emeritus', 'Controller', 'Controller', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Steven', 'Associate', 'Emeritus', 'Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Athletic', 'Degree', 'Registrar', 'Inclusion', 'Inclusion', 'Inclusion', 'Inclusion', 'Inclusion', 'Emeritus, Emeritus', 'Dean', 'Dean', 'Dean', 'Anthony', 'Anthony', 'Anthony', 'Dean, Dean, Dean, Dean', 'Carol', 'John', 'Associate, Associate, Associate', 'Dean', 'SPIA', 'Conduct', 'Conduct', 'Conduct', 'Conduct', 'Conduct', 'Conduct', 'Don', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Emeritus', 'Emergency', 'Emergency', 'Emerita, Emerita', 'Randy', 'forÂ', 'Richard', 'Gerald', 'Tom', 'Nicholson', 'Emerita', 'Dean', 'Isaac', 'Program', 'Jerry', 'Meg', 'Athletic', 'Moore', 'Dean, Dean, Dean', 'Tony', 'Tony', 'Tony', 'Tony', 'Dean', 'Dean', 'John', 'Darrell', 'Idonia, Sharon', 'Frederick', 'Josh', 'Frederick', 'Frederick', 'Josh', 'Athletic', 'Athletic', 'Sonya', 'Robert, Karen', 'Carrie', 'Carrie', 'Dean', 'Dean', 'Judith', 'Judith', 'Judith', 'Judith', 'Christina', 'Emeritus', 'Timothy, Diana', 'Rusty', 'Sunwoong', 'Steve', 'Randy', 'David', 'Emerita', 'Joan, Donald', 'Barry, Marcus', 'Barry, Marcus', 'Barry, Marcus', 'Barry, Marcus', 'Barry, Marcus', 'Barry, Marcus', 'Email', 'Registrar', 'Associate', 'Athletic, Athletic, Athletic', 'Athletic, Athletic, Athletic', 'Save', 'Athletic', 'Civic', 'Natalie', 'John', 'Retail', 'Residence', 'Residence', 'Residence', 'Residence', 'Residence', 'Residence', 'Athletic, Athletic, Athletic', 'Peter', 'Registrar', 'Brenda', 'Athletic', 'Chris', 'Barry, Steve', 'Derek', 'Derek', 'Emeritus', 'Heather, Heather', 'Peter', 'Bruce', 'Chuck', 'Tom', 'Registrar', 'Christa', 'Athletic, Athletic', 'Athletic, Athletic, Athletic, Athletic', 'Mike', 'Michael', 'Athletic', 'Athletic', 'Andrea', 'Emeritus', 'White', 'Emeritus, Emeritus', 'Sammy', 'Jenny', 'Retail', 'Contracts', 'Residence', 'Residence', 'Residence', 'Residence', 'Residence', 'Residence', 'Programming', 'Programmer, Programmer, Programmer', 'Stephen', 'Wojciech, Robert, Robert', 'David', 'Wins', 'Haefner', 'Dean', 'Mukul', '-, Athletic', 'Van, Van', 'David', 'Andrea', 'Andrea', 'Celia', 'Rebecca, Rebecca', 'McDonald, McDonald', 'Pemberton', 'Dean', 'Emeritus', 'ofÂ', 'Tracy', 'Registrar', 'Kasey', 'Burgess', 'Scott', 'Dean', 'Professor', 'Professor', 'Professor', 'John', 'Desmal', 'Mark, Alex', 'Emeritus', 'Engineer', 'Dean', 'Dean', 'Sanford, Michele', 'Wendy', 'Willie', 'Emeritus', 'Terry', 'Richard', 'Robert', 'Ian', 'Albanese', 'Ge', 'Mo', 'Bonacchi', 'Muller', 'Ryan', 'Gordon', 'Pluta', 'Papadimitriou', 'Greene', 'Cattani', 'Livnat, Livnat', 'Thomas', 'Emerita, Emerita', '&', 'Virginia', 'Controller', 'Dean', 'Rob, Wood, Evan', 'Alberto', 'Dean', 'Dean', 'Dean', 'John', 'Dean', 'Guerra', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Noelle', 'Jack', 'Michael', 'Tim', 'Tim', 'John', 'Thomas', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Hochschild', 'Diana, Gaddis', 'Dean, Athletic', 'Volleyball', 'Dean', 'Mary', 'Graglia', 'Dean, Dean', 'Dean', 'Jeff', 'Valarie', 'Yung, Jiguo', 'Dudley', 'Matthews', 'Nana', 'Dooley', 'David', 'Emerita', 'Cantens', 'Powlette', 'University', 'Emeritus', 'Naigles, Naigles', 'Roy, Roy', 'Bursar', 'Diana, Gaddis', 'Emeritus', 'Lev', 'Zarowin', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita', 'Hardart', 'Dean, Dean', 'Emeritus', 'Emerita', 'Emerita', 'Emeritus', 'Ana', 'Dean', 'Dean', 'McRae', 'Registrar', 'Controller', 'Controller', 'Controller', 'Controller, Controller', 'Controller, Controller', 'Controller, Controller', 'Dean', 'Direc', 'Jennifer', 'George', 'Dean, Dean', 'Rebecca', 'Ubuntu', 'Controller', 'Dean', 'Ben', 'Trump, Trump', 'Dean', 'Alex, Alex, Alex', 'Emeritus', 'Emeritus', 'Dean', 'Dean', 'Peery', 'Discusses, Discusses', 'Dean', 'Edwards', 'Gordon', 'Emeritus, Emeritus', 'Hall', 'Katz', 'Emeritus', 'Emeritus', 'Emeritus', 'Hoffman', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emerita', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Wendy', 'Dean', 'Jean', 'Jean, Tony', 'Jean, Tony', 'Gerhild', 'Richard, Diversity', 'Dean', 'Dean', 'Dean', 'Cruz', 'Appe', 'Lee', 'Sorensen', 'Breiner', 'Tim', 'Upward', 'Lance', 'Dean, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Dean, Dean', 'Dean', 'Dean', 'Yitzchak', 'Emeritus, Emeritus', 'Dean', 'â€“Â', 'â€“Â', 'â€“Â', 'â€“Â', 'Dean', 'Dean', 'Margaret', 'Highline', 'Highline', 'Dean, Athletic', 'Emeritus, Emeritus', 'Emerita', 'Michael, Morgan', 'Fossett', 'Lane, Lane', 'Lisa', 'Jess', 'Shaughnessy', 'Douglas, Marianne', 'Dean', 'William', 'Stuckey', 'Dean, Dean', 'John', 'Adjunct', 'Dean', 'Bryan', 'Emerit', 'Emeritus', 'Amy', 'Gateway', 'Pamela', 'Emeritus', 'Christakis', 'Breslin', 'Stewart', 'Refki', 'David', 'James', 'David', 'Yat', 'Yat', 'Dean', 'Dean', 'ofÂ', 'Emeritus', 'Suzyn', 'Baxter', 'Gerhild', 'Emeritus, Emerita, Emeritus, Emeritus', 'Dean', 'Hall', 'Emeritus, Emeritus, Emeritus, Elaina', 'Emerita', 'Emeritus', 'Hall', 'Batten', 'Dean', 'Athletic', 'Athletic', 'Athletic', 'Emerita', 'Career', 'Emeritus', '|', 'John', 'Desk, Desk', 'Dean', 'Johnson', 'Dean', 'Nancy', 'Burch', 'Emerita', 'Emeritus, Emeritus', 'Emeritus', 'Emerita', 'Emerita, Emerita', 'Emeritus', 'Emerita', 'Emerita', 'Emerita, Emerita', 'Emeritus', 'Emerita', 'Emeritus, Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita, Emerita', 'Emerita, Emerita', 'Emerita, Emerita', 'Emeritus', 'Emeritus, Emerita, Emerita', 'Emerita', 'Emerita, Emerita, Emerita', 'Emeritus, Emerita, Emerita', 'Emerita, Emerita', 'Emerita, Emerita', 'Emeritus', 'Emeritus', 'Emerita, Emerita', 'Emeritus', 'Emeritus', 'Emeritus', 'Emerita, Emerita', 'Emeritus, Emerita, Emerita', 'Emerita', 'Emeritus', 'Emerita, Emerita', 'Emeritus', 'Emeritus, Emeritus', 'Emerita, Emerita', 'Emeritus, Emeritus', 'Emerita', 'Emerita, Emerita', 'Emerita, Emerita', 'Emeritus, Emeritus', 'Emeritus', 'Emerita, Emerita', 'Emeritus', 'Emerita, Emerita', 'Emerita, Emerita', 'Emeritus, Emeritus, Emeritus', 'Emerita, Emerita', 'Emeritus', 'Emerita, Emerita', 'Emerita, Emerita', 'Emeritus', 'Emeritus', 'Emerita, Emerita', 'Dean', 'Dean', 'Dean', 'Dean', 'Amal, Shehata', 'Marciano', 'R., Kabaliswaran', 'Tom', 'Richard', 'Tuckman', 'Mueller', 'Lechner, Lechner', 'Dontoh', 'Guttman, Guttman', 'Emeritus, Eme', 'Allison', 'Younger', 'Jung, Jung', 'Dzuback', 'Schenkler', 'Hardart', 'David', 'David', 'David', 'David', 'David', 'Dean', 'Behrang', 'Registrar', 'Emeritus, Emerita', 'Emerita', 'Emerita, Emerita', 'Judy, Doris', 'Shelley, Grant', 'Carrie', 'Carrie', 'Jackie', 'Dean', 'Knight', 'Knight', 'Diversity', 'Ecoff', 'Dean', 'Bob, Ann', 'Bob, Ann', 'Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus', 'Ecoff', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus', 'Roster', 'Roster', 'Roster', 'Roster', 'Dean', 'Dean', 'Dean', 'Athletic', 'Tim', 'Lahra', 'Donald, Joe', 'Jim', 'Suzanne', 'Dean', 'Football', 'Scott', 'Jag', 'de', 'Stichting, Universiteit', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Athletic', 'Leslie', 'de', 'Dean', 'Josh', 'MBA', 'Athletic', 'Dean', 'Yung, Jiguo', 'Barry, Marcus', 'Kathleen', 'Kathleen', 'Kathleen', 'Dean', 'Perlick', 'Dean, Dean', 'Coordinator', 'Registrar', 'Michiyo, Okuhara', 'Jonathan', 'Dean', 'David', 'David', 'Sara', 'Sara', 'Rachel', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Hughes', 'Kolissa', 'Plunkett', 'Diversity, Bryan', '-Â', 'Terry', 'Max', 'Emeritus', 'Tracy', 'Dean', 'Diversity, Diversity', 'Dan', 'Chairperson, toÂ', 'Dean', 'Wenyika', 'Knight', 'ofÂ, ofÂ, ofÂ, ofÂ', 'ofÂ, ofÂ, ofÂ, ofÂ', 'ofÂ, ofÂ, ofÂ, ofÂ', 'ofÂ, ofÂ, ofÂ, ofÂ', 'ofÂ, ofÂ, ofÂ, ofÂ', 'Rudy, Adam', 'Athletic', 'George, Chinowsky', 'Wallace', '-', 'Registrar', 'Edelman', 'Athletic', 'Emeritus, Emeritus', 'Robert, Robert', 'Athletic, Athletic', 'Anthony', 'Emeriti', 'Registrar', 'Matt', 'Child', 'Bert', 'Abid, MENACA', 'James', 'James', 'Laboratory', 'Sara', 'Dean', 'Ed, Jen', 'Ed, Jen', 'Jameel', 'Jerrold', 'Emerita', 'Tom', 'Tom', '-', 'Emeritus', 'Tom', 'Emeritus', 'Football', 'Engineer, Engineer', 'Engineer', 'Martin', 'Tamar', 'Tamar', 'Tamar', 'Athletic, Athletic', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Pearsall, Pearsall', 'Thurman', 'Watts', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman, Registrar', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Thurman', 'Richard', 'Theresa, Theresa', 'Stephen', 'Anat', 'Athletic', 'Athletic', 'Thurman', 'Thurman, Registrar', 'Serene, Obama', 'Athletic, Athletic', 'Athletic, Athletic, Athletic', 'Dean', 'Dean', 'Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Athletic, Athletic', 'Phil', 'J.', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Anatomy', 'Anatomy', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean, Dean', 'Dean, Dean', 'Dean', 'Dean', 'Dean, Dean, Dean', 'Dean, Dean, Dean', 'Dean, Dean, Dean, Dean, Dean', 'Dean, Dean, Dean, Dean', 'Dean, Dean, Dean', 'Dean, Dean, Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Athletic', 'Athletic, Athletic, Athletic', 'Athletic', 'Amanda', 'Carrie', 'Dean', 'Dean', 'Profes', 'Emeritus', 'Emeritus', 'Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus', 'Emeritus, Emeritus, Emeritus', 'Emeritus, Emeritus', 'Kristina', 'Dean, Dean', 'Dean, Dean', 'Dean, Dean', 'Anatomy', 'Anatomy', 'Anatomy', 'Anatomy', 'Anatomy', 'Dean, Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean, Dean, Dean', 'Dean, Dean, Dean, Dean', 'Dean, Dean, Dean, Dean, Dean', 'Dean, Dean, Dean, Dean, Dean', 'Dean, Dean, Dean, Dean', 'Constance', 'Emeritus, Emeritus', 'Emeritus', 'Danielle', 'Jennifer', 'Nlandu, Moawia', 'Dean', 'Thurman', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Kasey', 'Tennis', 'Emeritus, Emeritus', 'Dean', 'Kevin, Jennifer', 'Jasmine', 'Thurman', 'Thurman', 'Thurman', 'Emerita', 'Philip', 'Physics', 'Physics', 'Physics', 'Physics', 'Carrie', 'Fischer', 'Lennard', 'Chen', 'Christina', 'Christina', 'Assael', 'Kovensky', 'Trope', 'Riccio', 'Williams, Williams', 'Balachandran', 'Frydman, Frydman', 'Kleinberger', 'Andrew, Harrison', 'Maheswaran', 'Melnick', 'Zin', 'Foudy', 'Greenleaf', 'Zaharoff', 'Bonezzi', 'Elton', 'Emeritus, Emeritus', 'Liberman', 'Dorobantu', 'Kim', 'Shi', 'Vickery', 'Armony, Armony', 'Koonin', 'Kruger', 'Levina', 'Emerita', 'Itzkowitz', 'Jung', 'Nayyar', 'Wong', 'Ghemawat', 'Lakner', 'Philip', 'Bartov', 'Athletic', 'Meng', 'Wurgler', 'Freeman', 'Howard', 'Magee', 'Naraya', 'Richardson', 'Tenenbein', 'David', 'David', 'Raghubir', 'Ronen', 'Singh, Singh', 'Dean, Dean', 'Emeritus', 'Peter', 'NEO', 'Dean', 'Dean', 'Michelle, Dena', 'Austin', 'NicolÃ¡s', 'Brenda', 'Athletic', 'Emeritus, Emeritus', 'Sharon', 'Laura', 'Provost', 'Emeritus', 'Emeritus', 'Emeritus', 'Straussman, Straussman', 'Weiss', 'Stazyk, Stazyk', 'Ben', 'Emeritus', 'Miller', 'John', 'Emeritus, Emeritus, Emeritus, Emeritus', 'Regarding, C.', 'Regarding, C.', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Dean', 'Craig, Matt', 'Craig, Matt', 'Dean', 'Sean', 'Regional', 'ofÂ', 'Dean', 'Richard', 'Football', 'Todd', 'Emeritus', 'Dean', 'Human', 'Human', 'Human', 'Keenan', 'Advancement', 'Programming', 'Public', 'Tamar', \"Billing, Cassagne, Diop, Rogers, Vitiello, O'Brien\", 'Athletic', 'John', 'forÂ', 'Emerita', 'Piero', 'Adjunct', 'Adjunct', 'Adjunct', 'Leigha', 'forÂ', 'Nana', 'III', 'Athletic', 'Nana', 'Nana', 'Kammer', 'Dean', 'Dean', 'Emerita', 'Dean', 'Andrew', 'Athletic', 'Barry', 'John, John, John', 'Kerry', '|', 'Nicholson']\n"
     ]
    }
   ],
   "source": [
    "print([data_filled['first_name'][idx] for idx in data_filled.index if data_filled['first_name'][idx]!='Not found'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, NER индентифицирует в качестве имён много других сущностей. Поэтому модель нуждается в дообучении: https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После удаления дубликатов, модель извлекла должности и имена сотрудников примерно из 5% записей.  \n",
    "\n",
    "Как показал анализ, записи содержат также другие варианты интересующих нас паттернов (фразы с другими комбинациями имён и должностей).\n",
    "\n",
    "Поскольку в скрипте использовалась дефолтная NER модель, качество выявления сущностей невысокое. \n",
    "\n",
    "Профессии могут распознаваться как часть имён и наоборот. \n",
    "\n",
    "ВОЗМОЖНОСТИ УСОВЕРШЕНСТВОВАНИЯ АЛГОРИТМА:\n",
    "\n",
    "1). Разработать базу из нескольких сотен примеров с именами и должностями, на которых обучить NER\n",
    "\n",
    "2). Написать ещё несколько вариантов паттернов для выявления должностей, например: \"Professor of History John Gray\", \"John Gray the Professor\". Для этого предварительно составить массив с потенциальными областями науки (история, математика и т.д). Можно разработать паттерны для трёхсложных имён.\n",
    "\n",
    "3). Если важна привязка к датам получения информации, можно не удалять из датасета дубликаты по столбцу json.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработаннный код в функциях значительно увеличивает время обработки датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57351\n",
      "['Brian', 'Mark']\n",
      "['Ray', 'Camolett']\n",
      "['Fenn']\n",
      "120 123 Director Brian Ray\n",
      "--- 0.04097723960876465 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Lists to store the extracted data\n",
    "l=len(data)\n",
    "print(l)\n",
    "job_title=[[] for _ in range(l)]\n",
    "data_first_names=[[] for _ in range(l)]\n",
    "data_last_names=[[] for _ in range(l)]\n",
    "spans=[]\n",
    "\n",
    "#Script timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#Test the script on the first row of dataset\n",
    "index_of_review_to_test_on = 0\n",
    "text_to_test_on = data.json.iloc[index_of_review_to_test_on]\n",
    "idx=index_of_review_to_test_on\n",
    "\n",
    "#Process text with nlp-model\n",
    "doc = nlp(text_to_test_on)\n",
    "\n",
    "#Recognize named entities in doc and create lists of names\n",
    "def get_names(doc):\n",
    "    persons=[entity.text for entity in doc.ents if entity.label_==\"PERSON\"]\n",
    "            \n",
    "    first_names=[person.split(' ')[0] for person in persons if ' ' in person]\n",
    "    last_names=[person.split(' ')[1] for person in persons if ' ' in person]\n",
    "    only_first_names=[person for person in persons if ' ' not in person]\n",
    "\n",
    "    return first_names,last_names,only_first_names\n",
    "\n",
    "first_names,last_names,only_first_names=get_names(doc)\n",
    "\n",
    "print(first_names) \n",
    "print(last_names)\n",
    "print(only_first_names)  \n",
    "\n",
    "#Create Matcher object        \n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#Create the patterns:\n",
    "def create_patterns(first_titles,last_titles,only_first_titles,first_names,last_names,only_first_names):\n",
    "     #1-word title+first_name+last_name\n",
    "    pattern1 = [{\"TEXT\": {\"IN\": only_first_titles}},{\"TEXT\":{\"IN\": first_names}},{\"TEXT\":{\"IN\":last_names}}]\n",
    "     #1-word title+1-word name (first_name)\n",
    "    pattern2=[{\"TEXT\": {\"IN\": only_first_titles}},{\"TEXT\":{\"IN\": only_first_names}}]\n",
    "     #2-words title+first_name+last_name\n",
    "    pattern3 = [{\"TEXT\": {\"IN\": first_titles}},{\"TEXT\": {\"IN\": last_titles}},{\"TEXT\":{\"IN\": first_names}},{\"TEXT\":{\"IN\":last_names}}]\n",
    "     #2-words title+1-word name (first_name)\n",
    "    pattern4=[{\"TEXT\": {\"IN\": first_titles}},{\"TEXT\": {\"IN\": last_titles}},{\"TEXT\":{\"IN\": only_first_names}}]\n",
    "\n",
    "    return pattern1,pattern2,pattern3,pattern4\n",
    "\n",
    "pattern1,pattern2,pattern3,pattern4=create_patterns(first_titles,last_titles,only_first_titles,first_names,last_names,only_first_names)\n",
    "\n",
    "#Add pattern rules to the matcher\n",
    "matcher.add(\"JSON\", None, pattern1,pattern2,pattern3,pattern4)\n",
    "\n",
    "#Find matches\n",
    "matches = matcher(doc)\n",
    "\n",
    "#Parse results to corrresponding lists\n",
    "def parse_matches(matches):\n",
    "    if not matches:\n",
    "                job_title[idx]='Not found'\n",
    "                data_first_names[idx]='Not found'\n",
    "                data_last_names[idx]='Not found'\n",
    "    else:\n",
    "                spans = [doc[start:end] for _, start, end in matches]\n",
    "                for span in spacy.util.filter_spans(spans): \n",
    "                    print(span.start, span.end, span.text)\n",
    "\n",
    "                    if span.end-span.start==2:  #1-word job-title, 1-word name (only first name)\n",
    "                        job_title[idx].append(doc[span.start])\n",
    "                        data_first_names[idx].append(doc[span.start+1]) \n",
    "                        data_last_names[idx].append('Not found')  \n",
    "                    elif span.end-span.start==4:  #2-words job-title, 2-words name (first name, last name)\n",
    "                        job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                        data_first_names[idx].append(doc[span.start+2]) \n",
    "                        data_last_names[idx].append(doc[span.start+3])\n",
    "                    else:  #Check if 2 first words are name of job_title             \n",
    "                        if str(doc[span.start:span.start+2]) in tokens:\n",
    "                            #2-word job-title, 1-words name (first name) \n",
    "                            job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                            data_first_names[idx].append(doc[span.start+2])  \n",
    "                            data_last_names[idx].append('Not found')  \n",
    "                        else:#1-word job-title, 2-words name (first name, last name)       \n",
    "                            job_title[idx].append(doc[span.start]) \n",
    "                            data_first_names[idx].append(doc[span.start+1]) \n",
    "                            data_last_names[idx].append(doc[span.start+2])\n",
    "                            \n",
    "    return job_title, data_first_names,data_last_names                        \n",
    "\n",
    "job_title[idx], data_first_names[idx],data_last_names[idx]=parse_matches(matches)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change parse_matches function for not printing the span.text\n",
    "def parse_matches(matches,idx):\n",
    "    idx=idx\n",
    "    if not matches:\n",
    "                job_title[idx]='Not found'\n",
    "                data_first_names[idx]='Not found'\n",
    "                data_last_names[idx]='Not found'\n",
    "    else:\n",
    "                spans = [doc[start:end] for _, start, end in matches]\n",
    "                for span in spacy.util.filter_spans(spans): \n",
    "                                            \n",
    "                    if span.end-span.start==2:  #1-word job-title, 1-word name (only first name)\n",
    "                        job_title[idx].append(doc[span.start])\n",
    "                        data_first_names[idx].append(doc[span.start+1]) \n",
    "                        data_last_names[idx].append('Not found')  \n",
    "                    elif span.end-span.start==4:  #2-words job-title, 2-words name (first name, last name)\n",
    "                        job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                        data_first_names[idx].append(doc[span.start+2]) \n",
    "                        data_last_names[idx].append(doc[span.start+3])\n",
    "                    else:  #Check if 2 first words are name of job_title             \n",
    "                        if str(doc[span.start:span.start+2]) in tokens:\n",
    "                            #2-word job-title, 1-words name (first name) \n",
    "                            job_title[idx].append(str(doc[span.start])+' '+str(doc[span.start+1]))\n",
    "                            data_first_names[idx].append(doc[span.start+2])  \n",
    "                            data_last_names[idx].append('Not found')  \n",
    "                        else:#1-word job-title, 2-words name (first name, last name)       \n",
    "                            job_title[idx].append(doc[span.start]) \n",
    "                            data_first_names[idx].append(doc[span.start+1]) \n",
    "                            data_last_names[idx].append(doc[span.start+2])\n",
    "                            \n",
    "    return job_title[idx], data_first_names[idx],data_last_names[idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(data)\n",
    "job_title=[[] for _ in range(l)]\n",
    "data_first_names=[[] for _ in range(l)]\n",
    "data_last_names=[[] for _ in range(l)]\n",
    "spans=[]\n",
    "\n",
    "for idx,string in data.iterrows():\n",
    "\n",
    "    doc = nlp(string.json)\n",
    "    first_names,last_names,only_first_names=get_names(doc)\n",
    "    pattern1,pattern2,pattern3,pattern4=create_patterns(first_titles,last_titles,only_first_titles,first_names,last_names,only_first_names)\n",
    "    matcher.add(\"JSON\", None, pattern1,pattern2,pattern3,pattern4)\n",
    "    matches = matcher(doc)\n",
    "    job_title[idx], data_first_names[idx],data_last_names[idx]=parse_matches(matches,idx) "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
